{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://ukb_data/phenomexcan/samples/50k/samples_50k_neale_eids.ht'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_JOBS = 30\n",
    "BLOCK_SIZE = 1024\n",
    "\n",
    "LD_BM_OUTPUT_DIR = 'gs://ukb_data/phenomexcan/ld/bm/'\n",
    "LD_TSV_OUTPUT_DIR = 'gs://ukb_data/phenomexcan/ld/tsv/'\n",
    "\n",
    "N_SAMPLE = 50000\n",
    "N_SAMPLE_SEED = 0\n",
    "SAMPLE_PREFIX = f'{int(N_SAMPLE / 1000)}k'\n",
    "SAMPLES_SAMPLED_OUTPUT = f'gs://ukb_data/phenomexcan/samples/{SAMPLE_PREFIX}/samples_{SAMPLE_PREFIX}_neale_eids.ht'\n",
    "display(SAMPLES_SAMPLED_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import subprocess\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 2.4.3\n",
      "SparkUI available at http://ukb-m.c.ukb-im.internal:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.22-597b3bd86135\n",
      "LOGGING: writing to /home/hail/hail-20190916-1400-0.2.22-597b3bd86135.log\n"
     ]
    }
   ],
   "source": [
    "import hail as hl\n",
    "hl.init(tmp_dir='gs://ukb_data/tmp/', min_block_size=BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs(bucket_name, prefix='', suffix='.*', delimiter=None):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    blobs = bucket.list_blobs(prefix=prefix, delimiter=delimiter)\n",
    "\n",
    "    return [f'gs://{bucket_name}/{b.name}' for b in blobs if re.search(suffix, b.name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://ukb_data/phenomexcan/regions_data/chr1_region0_variants.tsv',\n",
       " 'gs://ukb_data/phenomexcan/regions_data/chr1_region100_variants.tsv',\n",
       " 'gs://ukb_data/phenomexcan/regions_data/chr1_region101_variants.tsv',\n",
       " 'gs://ukb_data/phenomexcan/regions_data/chr1_region102_variants.tsv',\n",
       " 'gs://ukb_data/phenomexcan/regions_data/chr1_region103_variants.tsv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_blobs('ukb_data', prefix='phenomexcan/regions_data/chr1_', suffix='.tsv', delimiter='/')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to read variants in region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_variants_in_region(region_file):\n",
    "    variants_region = (\n",
    "        hl.import_table(region_file)\n",
    "    )\n",
    "    \n",
    "    return variants_region.annotate(v=hl.parse_variant(variants_region.variant)).key_by('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "# variants_region = _read_variants_in_region('gs://ukb_data/phenomexcan/regions_data/chr22_region1676_variants.tsv')\n",
    "\n",
    "# display(variants_region.show(5))\n",
    "\n",
    "# n_variants_region = variants_region.count()\n",
    "# print(n_variants_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to read genotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_bgen(chr_num, variants_region, samples_to_keep=None, sample_file='gs://ukb_data/genotypes/ukb19526_imp_chr1_v3_s487395.sample'):\n",
    "    bgen_file = f'gs://ukb_data/genotypes/v3/ukb_imp_chr{chr_num}_v3.bgen'\n",
    "    \n",
    "    bgen = hl.import_bgen(\n",
    "        path=bgen_file,\n",
    "        entry_fields=['GT'], #['GT', 'GP', 'dosage'],\n",
    "        sample_file=sample_file,\n",
    "        variants=variants_region.v,\n",
    "    )\n",
    "    \n",
    "    if samples_to_keep is None:\n",
    "        return bgen\n",
    "\n",
    "    return bgen.semi_join_cols(samples_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "# bgen = _read_bgen(22, variants_region)\n",
    "# display(bgen.describe())\n",
    "\n",
    "# n_bgen = bgen.count()\n",
    "# print(n_bgen)\n",
    "\n",
    "# display(bgen.show(5))\n",
    "# display(bgen.row.show(5))\n",
    "# display(bgen.col.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute LD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_variants_id(variants_data):\n",
    "    def _get_variantid(x):\n",
    "        return f'{x[\"locus.contig\"]}:{x[\"locus.position\"]}:{x[\"alleles\"][0]}:{x[\"alleles\"][1]}'\n",
    "    \n",
    "    _tmp = variants_data.rows().to_pandas()\n",
    "    _tmp = _tmp.assign(variantid=_tmp.apply(_get_variantid, axis=1))\n",
    "    return _tmp['variantid'].tolist()\n",
    "\n",
    "def _get_info_from_variant_file(variant_file):\n",
    "    info = {}\n",
    "    split = variant_file.split('/')[-1].split('_')\n",
    "    info['chr'] = split[0]\n",
    "    info['region'] = split[1]\n",
    "    info['chr_num'] = int(split[0].split('chr')[1])\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ld_and_save(variants_data, bm_out, tsv_out):\n",
    "    bgen_ld = hl.row_correlation(variants_data.GT.n_alt_alleles())\n",
    "    bgen_ld = bgen_ld.sparsify_triangle()\n",
    "    bgen_ld.write(bm_out, overwrite=True, force_row_major=True)\n",
    "    \n",
    "    hl.linalg.BlockMatrix.export(\n",
    "        path_in=bm_out,\n",
    "        path_out=tsv_out,\n",
    "        delimiter='\\t',\n",
    "        entries='strict_upper',\n",
    "        parallel=None,#'header_per_shard',\n",
    "        add_index=False,\n",
    "        header='\\t'.join(_get_variants_id(variants_data)),\n",
    "    )\n",
    "    \n",
    "def compute_ld_for_region(variant_file, samples, bm_out_dir, tsv_out_dir):\n",
    "    region_info = _get_info_from_variant_file(variant_file)\n",
    "    \n",
    "    var_data = _read_variants_in_region(variant_file)\n",
    "    bgen = _read_bgen(chr_num=region_info['chr_num'], variants_region=var_data, samples_to_keep=samples)\n",
    "    print(f'Region \"{region_info[\"region\"]}\" count: {bgen.count()}')\n",
    "    \n",
    "    bm_file = os.path.join(bm_out_dir, f'{region_info[\"chr\"]}_{region_info[\"region\"]}.bm')\n",
    "    tsv_file = os.path.join(tsv_out_dir, f'{region_info[\"chr\"]}_{region_info[\"region\"]}.tsv.bgz')\n",
    "    compute_ld_and_save(bgen, bm_file, tsv_file)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ld_complete(variant_files, samples, bm_out_dir, tsv_out_dir, n_jobs=1):\n",
    "    if n_jobs == -1:\n",
    "        n_jobs = 10\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
    "        tasks = {}\n",
    "        for var_file in variant_files:\n",
    "            future = executor.submit(\n",
    "                compute_ld_for_region,\n",
    "                var_file,\n",
    "                samples,\n",
    "                bm_out_dir,\n",
    "                tsv_out_dir,\n",
    "            )\n",
    "            tasks[future] = var_file\n",
    "        \n",
    "        # wait for tasks to finish\n",
    "        for future in as_completed(tasks):\n",
    "            var_file = tasks[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as ex:\n",
    "                print(f'Task on {var_file} finished with an exception: {ex}')\n",
    "            else:\n",
    "                var_file_info = _get_info_from_variant_file(var_file)\n",
    "                print(f'Region completed: {var_file_info[\"region\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_CHR = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['gs://ukb_data/phenomexcan/regions_data/chr22_region1676_variants.tsv',\n",
       " 'gs://ukb_data/phenomexcan/regions_data/chr22_region1677_variants.tsv',\n",
       " 'gs://ukb_data/phenomexcan/regions_data/chr22_region1678_variants.tsv',\n",
       " 'gs://ukb_data/phenomexcan/regions_data/chr22_region1679_variants.tsv',\n",
       " 'gs://ukb_data/phenomexcan/regions_data/chr22_region1680_variants.tsv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "variant_files = list_blobs('ukb_data', prefix=f'phenomexcan/regions_data/chr{SELECTED_CHR}_', suffix='.tsv', delimiter='/')\n",
    "display(len(variant_files))\n",
    "display(variant_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this list when running for a subset of regions\n",
    "# variant_files = [\n",
    "#     'gs://ukb_data/phenomexcan/regions_data/chr15_region1389_variants.tsv',\n",
    "#     'gs://ukb_data/phenomexcan/regions_data/chr15_region1401_variants.tsv',\n",
    "#     'gs://ukb_data/phenomexcan/regions_data/chr15_region1400_variants.tsv',\n",
    "#     'gs://ukb_data/phenomexcan/regions_data/chr15_region1415_variants.tsv',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50581\n"
     ]
    }
   ],
   "source": [
    "# read downsampled samples\n",
    "samples_downsampled = hl.read_table(SAMPLES_SAMPLED_OUTPUT)\n",
    "n_samples = samples_downsampled.count()\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:08 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "\n",
      "2019-09-16 18:11:18 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:18 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:18 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:18 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:18 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:18 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:18 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:18 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:18 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:18 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:18 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:18 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:19 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:20 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:20 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:20 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:21 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:39 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:39 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:39 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:40 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:40 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:40 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:40 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:40 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:40 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:40 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:40 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:40 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:40 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:40 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:40 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:41 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:42 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:43 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:43 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:43 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:43 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:43 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:43 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:43 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-09-16 18:11:43 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-09-16 18:11:43 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-09-16 18:11:51 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:11:51 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:11:51 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:11:51 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:11:52 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:11:52 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:11:52 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-09-16 18:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region \"region1693\" count: (3885, 50581)\n",
      "Region \"region1696\" count: (4426, 50581)\n",
      "Region \"region1683\" count: (4402, 50581)\n",
      "Region \"region1688\" count: (4521, 50581)\n",
      "Region \"region1691\" count: (3021, 50581)\n",
      "Region \"region1676\" count: (2114, 50581)\n",
      "Region \"region1684\" count: (4951, 50581)\n",
      "Region \"region1687\" count: (6870, 50581)\n",
      "Region \"region1677\" count: (2383, 50581)\n",
      "Region \"region1694\" count: (5070, 50581)\n",
      "Region \"region1699\" count: (5284, 50581)\n",
      "Region \"region1689\" count: (6735, 50581)\n",
      "Region \"region1690\" count: (4852, 50581)\n",
      "Region \"region1697\" count: (5146, 50581)\n",
      "Region \"region1678\" count: (4962, 50581)\n",
      "Region \"region1685\" count: (4935, 50581)\n",
      "Region \"region1686\" count: (3790, 50581)\n",
      "Region \"region1698\" count: (5885, 50581)\n",
      "Region \"region1681\" count: (3898, 50581)\n",
      "Region \"region1695\" count: (6082, 50581)\n",
      "Region \"region1679\" count: (5144, 50581)\n",
      "Region \"region1692\" count: (4370, 50581)\n",
      "Region \"region1682\" count: (6068, 50581)\n",
      "Region \"region1680\" count: (5491, 50581)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:19:45 Hail: INFO: Wrote all 13 blocks of 2383 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:19:50 Hail: INFO: Wrote all 26 blocks of 4521 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:19:51 Hail: INFO: Wrote all 13 blocks of 2114 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:19:59 Hail: INFO: Wrote all 13 blocks of 3021 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:20:14 Hail: INFO: wrote matrix with 2114 rows and 2114 columns as 1 block of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1676.bm\n",
      "2019-09-16 18:20:15 Hail: INFO: wrote matrix with 2383 rows and 2383 columns as 1 block of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1677.bm\n",
      "2019-09-16 18:20:35 Hail: INFO: Wrote all 26 blocks of 4426 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:20:36 Hail: INFO: merging 1 files totalling 18.8M...\n",
      "2019-09-16 18:20:37 Hail: INFO: Wrote all 26 blocks of 4951 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:20:37 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1676.tsv.bgz\n",
      "  merge time: 1.297s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:20:40 Hail: INFO: wrote matrix with 3021 rows and 3021 columns as 1 block of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1691.bm\n",
      "2019-09-16 18:20:42 Hail: INFO: merging 1 files totalling 24.1M...\n",
      "2019-09-16 18:20:43 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1677.tsv.bgz\n",
      "  merge time: 1.101s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:20:47 Hail: INFO: Wrote all 26 blocks of 6735 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:20:57 Hail: INFO: merging 1 files totalling 40.3M...\n",
      "2019-09-16 18:20:58 Hail: INFO: Wrote all 26 blocks of 4402 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:20:59 Hail: INFO: wrote matrix with 4521 rows and 4521 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1688.bm\n",
      "2019-09-16 18:20:59 Hail: INFO: Wrote all 26 blocks of 4852 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:21:00 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1691.tsv.bgz\n",
      "  merge time: 2.488s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:21:02 Hail: INFO: Wrote all 13 blocks of 3790 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:21:13 Hail: INFO: Wrote all 13 blocks of 3885 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:21:18 Hail: INFO: Wrote all 26 blocks of 5146 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:21:23 Hail: INFO: Wrote all 26 blocks of 5284 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:21:32 Hail: INFO: merging 2 files totalling 80.0M...\n",
      "2019-09-16 18:21:35 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1688.tsv.bgz\n",
      "  merge time: 2.930s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:21:41 Hail: INFO: Wrote all 26 blocks of 4370 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:21:43 Hail: INFO: Wrote all 26 blocks of 5070 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:21:44 Hail: INFO: Wrote all 13 blocks of 3898 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:21:44 Hail: INFO: wrote matrix with 4426 rows and 4426 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1696.bm\n",
      "2019-09-16 18:21:51 Hail: INFO: Wrote all 26 blocks of 6068 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:21:52 Hail: INFO: wrote matrix with 4951 rows and 4951 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1684.bm\n",
      "2019-09-16 18:22:07 Hail: INFO: wrote matrix with 3790 rows and 3790 columns as 1 block of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1686.bm\n",
      "2019-09-16 18:22:08 Hail: INFO: Wrote all 26 blocks of 5885 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:22:08 Hail: INFO: wrote matrix with 6735 rows and 6735 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1689.bm\n",
      "2019-09-16 18:22:08 Hail: INFO: wrote matrix with 4852 rows and 4852 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1690.bm\n",
      "2019-09-16 18:22:10 Hail: INFO: wrote matrix with 4402 rows and 4402 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1683.bm\n",
      "2019-09-16 18:22:14 Hail: INFO: merging 2 files totalling 85.4M...\n",
      "2019-09-16 18:22:17 Hail: INFO: wrote matrix with 3885 rows and 3885 columns as 1 block of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1693.bm\n",
      "2019-09-16 18:22:18 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1696.tsv.bgz\n",
      "  merge time: 3.589s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:22:21 Hail: INFO: Wrote all 26 blocks of 6870 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:22:26 Hail: INFO: merging 2 files totalling 102.5M...\n",
      "2019-09-16 18:22:28 Hail: INFO: merging 1 files totalling 53.8M...\n",
      "2019-09-16 18:22:28 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1684.tsv.bgz\n",
      "  merge time: 2.543s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:22:28 Hail: INFO: wrote matrix with 5146 rows and 5146 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1697.bm\n",
      "2019-09-16 18:22:29 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1686.tsv.bgz\n",
      "  merge time: 1.684s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:22:33 Hail: INFO: wrote matrix with 5284 rows and 5284 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1699.bm\n",
      "2019-09-16 18:22:37 Hail: INFO: Wrote all 26 blocks of 4935 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:22:43 Hail: INFO: merging 2 files totalling 86.0M...\n",
      "2019-09-16 18:22:46 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1683.tsv.bgz\n",
      "  merge time: 2.966s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:22:46 Hail: INFO: wrote matrix with 3898 rows and 3898 columns as 1 block of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1681.bm\n",
      "2019-09-16 18:22:49 Hail: INFO: merging 2 files totalling 99.7M...\n",
      "2019-09-16 18:22:50 Hail: INFO: wrote matrix with 4370 rows and 4370 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1692.bm\n",
      "2019-09-16 18:22:52 Hail: INFO: merging 1 files totalling 64.7M...\n",
      "2019-09-16 18:22:52 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1690.tsv.bgz\n",
      "  merge time: 3.025s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:22:53 Hail: INFO: Wrote all 26 blocks of 4962 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:22:55 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1693.tsv.bgz\n",
      "  merge time: 2.418s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:22:57 Hail: INFO: Wrote all 26 blocks of 5144 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:22:57 Hail: INFO: Wrote all 26 blocks of 6082 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:22:59 Hail: INFO: wrote matrix with 5070 rows and 5070 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1694.bm\n",
      "2019-09-16 18:22:59 Hail: INFO: wrote matrix with 6068 rows and 6068 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1682.bm\n",
      "2019-09-16 18:23:11 Hail: INFO: merging 2 files totalling 117.8M...\n",
      "2019-09-16 18:23:15 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1697.tsv.bgz\n",
      "  merge time: 3.820s\n",
      "2019-09-16 18:23:15 Hail: INFO: merging 2 files totalling 198.4M...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:23:16 Hail: INFO: merging 1 files totalling 62.7M...\n",
      "2019-09-16 18:23:18 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1681.tsv.bgz\n",
      "  merge time: 1.869s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:23:20 Hail: INFO: wrote matrix with 5885 rows and 5885 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1698.bm\n",
      "2019-09-16 18:23:20 Hail: INFO: merging 2 files totalling 78.8M...\n",
      "2019-09-16 18:23:21 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1689.tsv.bgz\n",
      "  merge time: 5.937s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:23:23 Hail: INFO: Wrote all 26 blocks of 5491 x 50581 matrix with block size 4096.\n",
      "2019-09-16 18:23:24 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1692.tsv.bgz\n",
      "  merge time: 3.684s\n",
      "2019-09-16 18:23:24 Hail: INFO: merging 2 files totalling 120.2M...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:23:28 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1699.tsv.bgz\n",
      "  merge time: 4.344s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:23:32 Hail: INFO: wrote matrix with 6870 rows and 6870 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1687.bm\n",
      "2019-09-16 18:23:43 Hail: INFO: merging 2 files totalling 115.6M...\n",
      "2019-09-16 18:23:46 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1694.tsv.bgz\n",
      "  merge time: 3.287s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:23:47 Hail: INFO: wrote matrix with 4935 rows and 4935 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1685.bm\n",
      "2019-09-16 18:23:52 Hail: INFO: merging 2 files totalling 159.1M...\n",
      "2019-09-16 18:24:01 Hail: INFO: wrote matrix with 4962 rows and 4962 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1678.bm\n",
      "2019-09-16 18:24:03 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1682.tsv.bgz\n",
      "  merge time: 11.303s\n",
      "2019-09-16 18:24:03 Hail: INFO: wrote matrix with 6082 rows and 6082 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1695.bm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:24:05 Hail: INFO: wrote matrix with 5144 rows and 5144 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1679.bm\n",
      "2019-09-16 18:24:09 Hail: INFO: merging 2 files totalling 154.5M...\n",
      "2019-09-16 18:24:14 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1698.tsv.bgz\n",
      "  merge time: 4.932s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:24:21 Hail: INFO: merging 2 files totalling 100.8M...\n",
      "2019-09-16 18:24:23 Hail: INFO: merging 2 files totalling 203.5M...\n",
      "2019-09-16 18:24:24 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1685.tsv.bgz\n",
      "  merge time: 3.216s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:24:29 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1687.tsv.bgz\n",
      "  merge time: 5.642s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:24:32 Hail: INFO: merging 2 files totalling 105.4M...\n",
      "2019-09-16 18:24:34 Hail: INFO: wrote matrix with 5491 rows and 5491 columns as 3 blocks of size 4096 to gs://ukb_data/phenomexcan/ld/bm/chr22_region1680.bm\n",
      "2019-09-16 18:24:35 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1678.tsv.bgz\n",
      "  merge time: 2.967s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:24:48 Hail: INFO: merging 2 files totalling 114.5M...\n",
      "2019-09-16 18:24:51 Hail: INFO: merging 2 files totalling 166.2M...\n",
      "2019-09-16 18:24:52 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1679.tsv.bgz\n",
      "  merge time: 4.163s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:24:57 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1695.tsv.bgz\n",
      "  merge time: 5.768s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:25:17 Hail: INFO: merging 2 files totalling 128.9M...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region completed: region1680\n",
      "CPU times: user 10.3 s, sys: 348 ms, total: 10.7 s\n",
      "Wall time: 14min 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-16 18:25:21 Hail: INFO: while writing:\n",
      "    gs://ukb_data/phenomexcan/ld/tsv/chr22_region1680.tsv.bgz\n",
      "  merge time: 4.663s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "compute_ld_complete(variant_files, samples_downsampled, n_jobs=N_JOBS,\n",
    "                   bm_out_dir=LD_BM_OUTPUT_DIR,\n",
    "                   tsv_out_dir=LD_TSV_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['gs://ukb_data/phenomexcan/ld/tsv/chr22_region1676.tsv.bgz',\n",
       " 'gs://ukb_data/phenomexcan/ld/tsv/chr22_region1677.tsv.bgz',\n",
       " 'gs://ukb_data/phenomexcan/ld/tsv/chr22_region1678.tsv.bgz',\n",
       " 'gs://ukb_data/phenomexcan/ld/tsv/chr22_region1679.tsv.bgz',\n",
       " 'gs://ukb_data/phenomexcan/ld/tsv/chr22_region1680.tsv.bgz']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bm_list = list_blobs('ukb_data', prefix=f'phenomexcan/ld/tsv/chr{SELECTED_CHR}_', suffix='.tsv.bgz', delimiter='/')\n",
    "display(len(bm_list))\n",
    "display(bm_list[:5])\n",
    "assert len(bm_list) == len(variant_files), (len(bm_list), len(variant_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
