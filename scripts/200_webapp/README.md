# ShinyApp for PhenomeXcan
These steps read the S-PrediXcan and S-MultiXcan results and load all the needed data into a PostgreSQL database for
querying from a ShinyApp.

## Configure paths
Open the file `settings.sh` and configure where the different result files are located, as well as the PostgreSQL
connection options. Once that's done, in your terminal type:
```bash
$ . settings.sh
```
In the same terminal follow the steps below.

## Create a new PostgreSQL database
```bash
$ psql -v db_name=${DBDATABASE_NAME} -h ${DBHOST} -U ${DBUSER} -p ${DBPORT} -f postgresql/create_database.sql
```

## Create common tables

```bash
$ psql -h ${DBHOST} -U ${DBUSER} -p ${DBPORT} -d ${DBDATABASE_NAME} -f postgresql/load_phenomexcan_phenos.sql
...

$ psql -h ${DBHOST} -U ${DBUSER -p ${DBPORT} -d ${DBDATABASE_NAME} -f postgresql/load_genes.sql
...

$ psql -h ${DBHOST} -U ${DBUSER -p ${DBPORT} -d ${DBDATABASE_NAME} -f postgresql/load_tissues.sql
```

# S-PrediXcan
1. Make sure the S-PrediXcan results were postprocessed by running this notebook:
```bash
$ cd scripts/
$ bash run_nb.sh 100_postprocessing/05_spredixcan.ipynb
```
This will generate a set of HDF5 files for easy reading in the next steps.

1. Run `combine_spredixcan_results.sh` to generate a single `.tsv` file for each tissue with all the S-PrediXcan
results (this will be used later to load into PostgreSQL).
```bash
$ bash combine_spredixcan_results.sh /mnt/tmp/spredixcan_results 12
Output folder: /mnt/tmp/spredixcan_results
Using n jobs: 12
Thyroid - 0
Skin_Not_Sun_Exposed_Suprapubic - 1
Colon_Transverse - 2
Brain_Amygdala - 3
Ovary - 4
Colon_Sigmoid - 5
...
```
where `/mnt/tmp/spredixcan_results` is the output folder and `12` the number of cores/jobs.

1. Make sure the output files were correctly created:
```bash
$ cat /mnt/tmp/spredixcan_results/Thyroid.tsv | awk -F'\t' '{print NF-1}' | sort | uniq -c
61218228 5
```
A correct output must show a single line (for example, 5 tabs per line, as shown above).

1. Load the data into a local PostgreSQL database (preferably version 11). Check the `.sql`
files to verify the file paths.
```bash
$ cat ${TISSUES_FILE} | cut -f2 | tail -n +2 | \
    parallel -j8 'psql -h ${DBHOST} -U ${DBUSER} -p ${DBPORT} -d ${DBDATABASE_NAME} -c "\copy spredixcan from /mnt/tmp/spredixcan_results/{}.tsv with delimiter as E'"'"'\t'"'"' csv header"'
```
Remember to change `/mnt/tmp/spredixcan_results` by the output folder you used.

1. For the S-PrediXcan results, given the large data, you have to first create the tables by running:
```bash
$ psql -h ${DBHOST} -U ${DBUSER} -p ${DBPORT} -d ${DBDATABASE_NAME} -f postgresql/load_spredixcan.sql
```

1. Then load the data with:
```bash
$ cat ${TISSUES_FILE} | cut -f2 | tail -n +2 | \
    parallel -j8 'psql -h ${DBHOST} -U ${DBUSER} -p ${DBPORT} -d ${DBDATABASE_NAME} \
    -c "\copy spredixcan from /mnt/tmp/spredixcan_results/{}.tsv with delimiter as E'"'"'\t'"'"' csv header"'    
```
where the file `${TISSUES_FILE}` is generated by the notebook `100_postprocessing/01_genes_mappings.ipynb`.


# PhenomeXcan and ClinVar
```bash
$ psql -h ${DBHOST} -U ${DBUSER} -p ${DBPORT} -d ${DBDATABASE_NAME} -f postgresql/load_clinvar_phenos.sql
DROP TABLE
CREATE TABLE
COPY 5106

$ psql -h ${DBHOST} -U ${DBUSER} -p ${DBPORT} -d ${DBDATABASE_NAME} -f postgresql/load_ukb_clinvar.sql
DROP TABLE   
CREATE TABLE 
COPY 20888646
CREATE INDEX 
...
```


# S-MultiXcan

1. Generate a file with all results:
```bash
$ bash combine_smultixcan_results.sh /mnt/tmp/output.tsv 4
Output file: /mnt/tmp/output.tsv
Using n jobs: 4
Adding header
Adding data
```
where `/mnt/tmp/output.tsv` is the output file and `4` the number of jobs.

1. Make sure the output file was correctly created:
```bash
$ cat /mnt/tmp/output.tsv | awk -F'\t' '{print NF-1}' | sort | uniq -c
91055811 12
```
A correct output must show a single line (for example, 10 tabs per line, as shown above).

1. Now generate the needed files for the UK Biobank/ClinVar table by running:
```bash
$ bash ../run_nb.sh 10_generate_ukb_clinvar_table.ipynb
```

1. Load the data into a local PostgreSQL database (preferably version 11). Check the `.sql`
files to verify the file paths.
```bash
$ export DBUSER="your_postgresql_user"
$ export DBPORT="5432"
$ psql -h ${DBHOST} -U ${DBUSER} -p ${DBPORT} -d ${DBDATABASE_NAME} -f postgresql/load_smultixcan.sql
DROP TABLE
CREATE TABLE
COPY 91055810
CREATE INDEX
...
```


# Database dump

1. Dump the local PostgreSQL database:
```bash
$ pg_dump -h ${DBHOST} -U ${DBUSER} -p ${DBPORT} -d ${DBDATABASE_NAME} \
  --format=plain --no-owner --no-acl \
  | sed -E 's/(DROP|CREATE|COMMENT ON) EXTENSION/-- \1 EXTENSION/g' \
  | gzip > phenomexcan_db.sql.gz
```
You can replace `gzip` with `pigz` for parallel compression.


# Cloud SQL

1. Create a Cloud SQL instance:
```bash
$ export INSTANCE_NAME="phenomexcan-prod-v01"
$ gcloud sql instances create $INSTANCE_NAME \
    --database-version=POSTGRES_11 \
    --tier db-f1-micro \
    --region="us-central1" \
    --storage-auto-increase \
    --availability-type=ZONAL \
    --authorized-networks=54.204.34.9,54.204.36.75,54.204.37.78,34.203.76.245,3.217.214.132,34.197.152.155 \
    --storage-size=40
```

The parameter `--authorized-networks` are for allowing Shinyapp to make queries (taken from
[here](https://docs.rstudio.com/shinyapps.io/applications.html#firewalls)):

```bash
$ read -s PASSWORD # enter your password here
$ gcloud sql users set-password postgres no-host --instance=$INSTANCE_NAME \
       --password=$PASSWORD
```

```bash
$ gcloud sql instances patch $INSTANCE_NAME --database-flags temp_file_limit=5048576
```

1. Upload the dump file to a Google Cloud bucket (these steps were taken from the
user guide of Cloud SQL):
```bash
# upload file to google cloud
gsutil cp phenomexcan_db.sql.gz gs://phenomexcan/

# get Cloud SQL instance info (serviceAccountEmailAddress)
gcloud sql instances describe $INSTANCE_NAME | grep serviceAccountEmailAddress

# add the service account to the bucket ACL as a writer
gsutil acl ch -u [SERVICE_ACCOUNT_ADDRESS]:W gs://phenomexcan

# add the service account to the import file as a reader
gsutil acl ch -u [SERVICE_ACCOUNT_ADDRESS]:R gs://phenomexcan/phenomexcan_db.sql.gz

# TODO: create remote database? maybe it's already present in phenomexcan_db.sql.gz

# import the data
$ export CLOUD_SQL_DB_NAME="phenomexcan"
$ gcloud sql import sql $INSTANCE_NAME \
    gs://phenomexcan/phenomexcan_db.sql.gz \
    --database=${CLOUD_SQL_DB_NAME} \
    --user=postgres
```

Finally, inside a Cloud SQL shell, update the stats of the database so the indexes
are correctly used:
```bash
vacuum analyze;
```


## Shinyapp deploy

Follow the steps described in the `shinyapp` folder.
